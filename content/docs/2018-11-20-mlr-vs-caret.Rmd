---
title: "mlr vs. caret"
authors: ["philipp-probst"]
date: 2018-11-20
categories: ["R", "r-bloggers"]
output:
  blogdown::html_page:
    toc: true
tags: ["mlr", "caret", "machine learning", "R"]
---

Let`s compare the two popular R packages for machine learning [**mlr**](https://github.com/mlr-org/mlr) and [**caret**](https://github.com/topepo/caret).

**caret** is longer on the market, its first CRAN release is from 2007, while **mlr** came to CRAN on 2013. 
As for now, **caret** seems to be more popular, according to [**cranlogs**](https://cran.r-project.org/web/packages/cranlogs/index.html). 
**caret** was downloaded 178029 times while **mlr** sums up to 11408 downloads in the last 30 days.

<!--excerpt-->

The purpose of the packages seems to be quite similar. The **caret** [website](https://github.com/topepo/caret) defines **caret** as follows:

> *The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models.*

The purpose of **mlr** is described on their [website](https://github.com/topepo/caret):

> *R does not define a standardized interface for all its machine learning algorithms. Therefore, for any non-trivial experiments, you need to write lengthy, tedious and error-prone wrappers to call the different algorithms and unify their respective output. [. . .] The framework provides supervised methods like classification, regression and survival analysis along with their corresponding evaluation and optimization methods, as well as unsupervised methods like clustering. It is written in a way that you can extend it yourself or deviate from the implemented convenience methods and construct your own complex experiments or algorithms.*

Both packages aim to automatize processes for standard tasks like classification and regression and make life easier for data scientists.
So which package to use?
This post aims on doing a fair comparison between these two packages. 
As the author of this post contributed to the development of **mlr** the view may be biased.

# Installation

Both packages heavily depend on other packages and hence installation takes a long time. 
I tested both packages on my Windows machine with a fresh R version (3.5.1 at the time writing) and without having installed any dependencies beforehand. 
For **caret** it took 140 seconds to install while **mlr** needed 46 seconds (without including suggested packages).

# Documentation

Both have an online tutorial. 
[**caret**`s one](http://topepo.github.io/caret/index.html) is good for learning from scratch, but for looking things up quickly it is rather unpractical. 
Here lies the strength of the [**mlr** tutorial](https://mlr.mlr-org.com/index.html).

**caret** provides more things. There is the [Applied Predictive Modelling book](https://www.springer.com/de/book/9781461468486) which tries to *cover the overall predictive modeling process.* It is not freely available online, like, for example, some books of Hadley Wickham (e.g., [Advanced R](http://adv-r.had.co.nz/), [R for Data Science](https://r4ds.had.co.nz/) or [R packages](http://r-pkgs.had.co.nz/)) or [The elements of statistical learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf).

Moreover there is a [datacamp course](https://www.datacamp.com/courses/machine-learning-toolbox) that uses **caret**.
Cheatsheets are available for both. The one for [**mlr**](https://github.com/rstudio/cheatsheets/raw/master/mlr.pdf) seems to be more extensive and detailed than the [**caret**](https://github.com/rstudio/cheatsheets/raw/master/caret.pdf) one.

**All in all, especially because of the nice book, caret seems to be the winner here: 0:1**

# Simple code example

To compare the two packages directly here you can see the standard codes of lines that are necessary to perform a simple 5-fold cross-validation for the boston housing dataset using the random forest package [**ranger**](https://github.com/imbs-hl/ranger).

**mlr**:

```{r, eval = FALSE}
library(mlr)
data(BostonHousing, package = "mlbench")
regr.task = makeRegrTask(data = BostonHousing, target = "medv")
lrn = makeLearner("regr.ranger")
rdesc = makeResampleDesc("CV", iters = 5)
res = resample(learner = lrn, task = regr.task, resampling = rdesc, measure = list(rmse))
res
```

**caret**:

```{r, eval = FALSE}
library(caret)
data(BostonHousing, package = "mlbench")
fitControl = trainControl(method = "cv", number = 5)
fit = train(medv ~ ., data = BostonHousing, method = "ranger", trControl = fitControl)
fit
```

In this code you can already see a crucial difference. 
In **mlr** you have to create a task and a learner. 
This means more code writing, but gives the user also more flexibility and overview of what is being done. 
In **caret** you can pass the data and the method directly to the `train()` function and internally some tuning is done for the user without having the possibility to adjust this easily.

This behavior can be changed in both packages, e.g., by directly passing the character `"regr.ranger"` as the learner argument to `train()` or by creating a custom learner in **caret** as described [here](http://topepo.github.io/caret/using-your-own-model-in-train.html).

**No clear winner here: 1:2**

# Features

Let`s compare the different features that the two packages have to offer for different issues of the machine learning process.

## Preprocessing

Both packages have several possibilities for preprocessing the data. 
Standard procedures like dummy feature creation (e.g. for categorical predictors), normalization and imputation and feature selection methods are included. 
In **caret** also standard transformations (scaling, PCA, Box-Cox, Yeo-Johnson, etc.) can be used. 
An advantage of **mlr** is, that all preprocessing methods of **caret** can be used via the wrapper function `makePreprocWrapperCaret()`. 
Both provide the option to use these preprocessing steps also in the resampling process.

**No clear winner here: 2:3**

## Tasks

**mlr** supports a larger variety of tasks, such as classification, regression, survival, clustering, multilabel, cost-sensitive, imbalanced data, functional data and spatial data.

**caret** only seems to have possibilities for Classification, Regression and Cost-sensitive.

**mlr is better here: 3:3**

## Learners

Currently 271 learners are supported in **caret**. 
For **mlr** there are *only* 165. 
However, the difference between both mainly breaks down to niche learners. 
The most used algorithms are available in both packages.

**mlr** provides an easy way to look up the learners and their properties via ```listLearners()``` or via the [tutorial]((https://mlr.mlr-org.com/index.html)) in a table format. Learners for **caret** can be looked up in the tutorial, [chapter 7](http://topepo.github.io/caret/train-models-by-tag.html).
Both packages provide the possibility to extend the learners.

What is clearly missing is a list of recommended learners in both packages. New users just have to look for the **standard** packages, without knowing if they are good or if there are possibly better implementations.

**No clear winner here, although documentation seems to be better for mlr here: 4:4**

## Resampling strategies

There are several resampling strategies available in both packages. Of course both include the most standard procedure (repeated) k-fold cross-validation. Moreover the strategies holdout, bootstrap and leave-one-out are available for both. Both have the option for growing window cross-validation which is especially useful for time series data. **caret** has additionally the possibility to use out-of-bag methods that are available for methods that use bagging like e.g. random forest. This is not available for **mlr**, only the out-of-bag predictions of a normally trained random forest model can be extracted here.

**Small advantage for caret: 4:5**

## Measures

In **mlr** there are currently [71 measures](https://mlr.mlr-org.com/articles/tutorial/measures.html) implemented for all the different tasks that can be handled in **mlr**. 
Custom measures can be added by the user (see [here](https://mlr.mlr-org.com/articles/tutorial/create_measure.html)). 
Only one measure is given as default (Mean Squared Error (`mse`) for regression and the Mean Misclassification Error (`mmce`) for classification).

**caret** has two standard measures that are used for each of the tasks (Root Mean Square Error (RMSE) and RÂ² (Rsquared) for regression and Accuracy and Kappa for classification). 
They are changeable via the `trainControl` argument but it is more complicated than with **mlr**.

**mlr wins here: 5:6**

## Benchmarking

**mlr** provides the function ```benchmark``` for executing a comparison of several learners and several datasets/tasks at once (even in parallel). No such functionality is available in **caret**.

**6:6**

## Tuning

In **mlr** tuning is not done automatically. 
There are functions and wrappers to perform the tuning. 
Supported tuning methods are _random search_, _grid search_, _F-Racing_ and _model-based optimization_ via the **mlrMBO** package. 
Here is a code example with a simple tuning of a support vector machine:

```{r, eval = FALSE}
library(mlr)
data(BostonHousing, package = "mlbench")
regr.task = makeRegrTask(data = BostonHousing, target = "medv")

discrete_ps = makeParamSet(
  makeDiscreteParam("C", values = c(0.5, 1.0, 1.5, 2.0)),
  makeDiscreteParam("sigma", values = c(0.5, 1.0, 1.5, 2.0))
)
ctrl = makeTuneControlGrid()
rdesc = makeResampleDesc("CV", iters = 3L)
res = tuneParams("regr.ksvm", task = regr.task, resampling = rdesc,
  par.set = discrete_ps, control = ctrl)
res
```

The tuning strategy is defined in `control`. 
For a nested tuning design a learner has to be wrapped via `makeTuneWrapper()`.

Tuning is done internally and automatically in the **caret** package. 
What the tuning does is described in the [learner section](http://topepo.github.io/caret/train-models-by-tag.html) of the tutorial. 
This can be changed by the user to _grid search_ or _random search_ in `trainControl()`:

```{r, eval = FALSE}
library(caret)
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9),  n.trees = (1:30)*50, 
                        shrinkage = 0.1,  n.minobsinnode = 20)
fitControl = trainControl(method = "cv", number = 5, search = "grid")
fit = caret::train(medv ~ ., data = BostonHousing, method = "gbm", trControl = fitControl)
fit
```

For both packages there are plots to visualize the tuning process, for example to plot the tuning parameters (1 or 2) vs. the performance or for plotting the development of the performance in the tuning iteration steps. 

**Light advantage for mlr here as it supports advanced tuning algorithms like F-Racing and model-based optimization. Executing the tuning is easier in caret: 7:7**

## Visualization of results

Visualizing your results and getting information about the models is an important task in the modeling pipeline. 
For both packages we have ROC-Analysis and calibration curves. 
**mlr** provides additionally partial dependence plots, learning curves, cut-off curves and residual plotting, while **caret** comes with specific variable importance methods and lift curves.

**No clear winner here: 8:8**

## Datasets

Datasets can be nice for illustration and testing purposes.
**caret** comes with 10 datasets, which are nicely [documented](http://topepo.github.io/caret/data-sets.html).
**mlr** provides 16 already built tasks. Moreover in **mlr** there is a nice connection to the [**OpenML**](https://github.com/openml/openml-r) package and its [online platform](https://www.openml.org/) with over 20000 datasets from all domains.

**Advantage for mlr here: 9:8**

# Speed

There shouldn`t be a big time difference for the operations of the package itself as most of the operations are not computationally intensive.

## Difference in the default mode

**caret** has prespecified tuning if the learners are used in their default version so it will take more time than **mlr** for which tuning is not done in the default method. In the first example above **mlr** had a runtime of 2.4 seconds while **caret** took 3.2 seconds. The difference will be much larger, when bigger datasets are used. In general the automatic tuning of **caret** will also lead to better performance.

**No clear winner here: 10:9**

## Parallelization

Parallelization in the **caret** package only works internally when applying a tuning strategy like grid search by using the [**doParallel**](http://topepo.github.io/caret/parallel-processing.html) package which is based on [**snow**](https://cran.r-project.org/web/packages/snow/index.html).

In contrast in **mlr** offers more options. 
Parallelization can be done on different [levels](https://mlr.mlr-org.com/articles/tutorial/parallelization.html): 
- when benchmarking different learners and datasets, 
- when resampling
- when selecting features
- while tuning hyperparameters and 
- while training an ensemble. 

Parallelization is done via [**parallelMap**](https://cran.r-project.org/web/packages/parallelMap/index.html) which works with all major parallelization backends: [**parallel**](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf), [**snow**](https://cran.r-project.org/web/packages/snow/index.html) or [**batchtools**](https://cran.r-project.org/web/packages/batchtools/batchtools.pdf). Especially useful is the function ```batchmark``` which is good for parallelizing benchmarks via [**batchtools**](https://cran.r-project.org/web/packages/batchtools/batchtools.pdf).

**mlr has more possibilities here: 11:9**

# Objects and configurability

In **mlr** you have an object for every part of the process: a task, a learner, a measure, a resample object. 
In **caret** you have the ```trainControl``` object that is used in `train()`, but it feels that **mlr** is more intuitive and clear here.
Because of this it is also easier in **mlr** to integrate new custom learners, measures, filters and imputation methods.

**mlr wins here: 12:9**

# Further development

**caret** will be retired in future. New developments include the packages **recipes**, **yardstick**, **infer**, **parsnip** and are all part of [**tidymodels**](https://www.tidyverse.org/articles/2018/08/tidymodels-0-0-1/).

**mlr** developers are currently working on [**mlr3**](https://github.com/mlr-org/mlr3) which aims at being even more extensible and uses **R6**, **data.table**, **future** and other useful packages that were not used when **mlr** was released back in the years.

# Overall result

Counting together all categories **mlr** wins by 12:9.

<!--## Overview in table --> 

## Thanks
Thanks to [Rahul Sangole (@rsangole)](https://github.com/rsangole), who helped with information about the **caret** package.
This blog post also appears on the [personal blog site](http://philipppro.github.io/) of @PhilippPro.
